\documentclass{beamer}

% Top-aligning columns within a top-aligned frame
% https://tex.stackexchange.com/questions/16447/beamer-top-aligning-columns-within-a-top-aligned-frame
\makeatletter
\newenvironment{myitemize}{%
   \setlength{\topsep}{0pt}
   \setlength{\partopsep}{0pt}
   \renewcommand*{\@listi}{\leftmargin\leftmargini \parsep\z@ \topsep\z@ \itemsep\z@}
   \let\@listI\@listi
   \itemize
}{\enditemize}
\makeatother  

\usepackage[USenglish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb, amsmath}
\usepackage{bm}
\usepackage{color}
\usepackage{tikz}
\usepackage{url}

\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks,linkcolor=,urlcolor=links}

\usetheme{Boadilla}

\bibliographystyle{apalike}
% make bibliography entries smaller
%\renewcommand\bibfont{\scriptsize}
% Now get rid of all the colours
\setbeamercolor*{bibliography entry title}{fg=black}
\setbeamercolor*{bibliography entry author}{fg=black}
\setbeamercolor*{bibliography entry location}{fg=black}
\setbeamercolor*{bibliography entry note}{fg=black}

\newcommand{\lnorm}[1]{\left\lVert#1\right\rVert^2}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% and kill the abominable icon
\setbeamertemplate{bibliography item}{}

\begin{document}
\title[NBDT]{NBDT: Neural-Backed Decision Trees}  
\author{Radek Bartyzal}
\date{5. 5. 2020} 
\institute{GLAMI AI}

\frame{\titlepage} 

\begin{frame}{Motivation}

Interpretability of models:
\begin{itemize}
\item decision trees = good
\item neural nets = bad
\end{itemize}

\vfill

Saliency maps:
\begin{itemize}
\item tells you what the nets is "looking" at
\item good for debugging = is the net focusing on the right object?
\item does not help if the net looks at the right object but predicts wrong class
\end{itemize}

\end{frame}

%--------- END Frame 12 -------------


%--------- END Frame 12 -------------
\begin{frame}{Overview}

\begin{figure}[h]
\includegraphics[width=\textwidth]{img/train}
\caption{}
\end{figure}

\end{frame}

%--------- END Frame 12 -------------
\begin{frame}{Prediction}

\begin{itemize}
\item get embedding of the sample = $x$ = output of the pre-last layer
\item take the last weight matrix $W$ of the net = producing the prediction probabilities
\item each column $w_i$ corresponds to one output = class
\item each class $c$ = leaf node $c$ $\implies$ $w_c$ = $r_c$ = representation vector of node $c$  
\item probability of node $n$ = $<x,r_n>$ = for leaf node $c$ = $<x, w_c>$
\item representation vector of inner node = average of repr. vectors of child nodes
\end{itemize}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Soft vs Hard prediction}

\begin{figure}[h]
\includegraphics[width=\textwidth]{img/hard_soft}
\end{figure}

\begin{itemize}
\item Hard: select argmax of nodes at current level and continue only with the winners children nodes
\item Soft: softmax of nodes at each level => calculate probability of path from the leaf to the root and select the leaf with highest path probability
\item Naive: does not support multiple levels
\end{itemize}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Training}

\begin{itemize}
\item pre-train the model on the dataset
\item construct the nodes by hierarchical clustering done from the $w_i$ columns of the final weight matrix = get hierarchy of nodes = decision tree
\item fine-tune the model on the fixed hierarchy to cluster the $w_i$ of the parent nodes together better
\end{itemize}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Inducing = building hierarchy of nodes}

\begin{figure}[h]
\includegraphics[width=\textwidth]{img/build}
\caption{\textbf{A.} Load the weights of pre-trained
neural network’s final fully-connected layer = $W$. \textbf{B.}
Use each column $w_i$ of $W$ as representative vectors for each leaf node. \textbf{C.} Use the average of each
pair of leaves for the parents’ representative vectors. \textbf{D.} For each ancestor, take the
subtree it is the root for. Average representative vectors for all leaves in the subtree.
That average is the ancestor’s representative vector.}
\end{figure}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Fine-tuning is not essential}

\begin{figure}[h]
\includegraphics[width=\textwidth]{img/loss}
\end{figure}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Interpretability}

\begin{figure}[h]
\includegraphics[width=\textwidth]{img/interpret}
\caption{The ResNet10 hierarchy makes less sense than the WideResNet hierarchy. In this hierarchy, Cat, Frog, and Airplane are placed under the same subtree. The WideResNet hierarchy cleanly splits Animals and Vehicles, on each side of the hierarchy.}
\end{figure}

\end{frame}
%--------- END Frame 12 -------------
\begin{frame}{Conclusion}

\begin{itemize}
\item hierarchical clustering of the columns $w_i$ of the last weight matrix $W$
\item check if the clustering makes intuitive sense = animals / vehicles in different subtrees
\item visualize tree traversal paths = find the most frequently traversed incorrect paths $\implies$ e.g. see dependency on the background: both fish and ships have sea
\end{itemize}

\end{frame}
%--------- END Frame 12 -------------


\begin{frame}{Sources}

\begin{thebibliography}{0}

  \bibitem[1]{cit:paper} 1. Wan, Alvin, et al. "NBDT: Neural-Backed Decision Trees." arXiv preprint arXiv:2004.00221 (2020). \url{https://arxiv.org/abs/2004.00221} 
  
  \bibitem[2]{cit:blog} 2. Blog post with links to code. \url{https://bair.berkeley.edu/blog/2020/04/23/decisions/}
  
\end{thebibliography}

\end{frame}

 
\end{document}
